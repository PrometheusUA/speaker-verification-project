{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese CNN training for speaker verification\n",
    "\n",
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\_UNIVER\\UCU\\1 sem\\Linear algebra\\Project\\speaker-verification-project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import timm\n",
    "import gc\n",
    "\n",
    "from torchaudio.datasets import VoxCeleb1Verification\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from IPython.display import display, Audio\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda import empty_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = './models/efficientnet_1.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = VoxCeleb1Verification('../data', download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split\n",
    "\n",
    "We need to split data in the clever way, leaving some speakers for validation, and not using them as well as some speakers from pre-defined test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# occs = dict()\n",
    "# for _, _, _, _, f1, f2 in tqdm(test_dataset):\n",
    "#     id1 = f1.split('-')[0]\n",
    "#     id2 = f2.split('-')[0]\n",
    "#     if id1 not in occs:\n",
    "#         occs[id1] = 0\n",
    "#     if id2 not in occs:\n",
    "#         occs[id2] = 0\n",
    "#     occs[id1] += 1\n",
    "#     occs[id2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speakers who are used in test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(occs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ids = sorted(list(occs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting validation speakers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_ids = np.random.choice(list(set(os.listdir('../data/wav/')) - set(test_ids)), size=40, replace=False)\n",
    "# val_ids  = sorted(list(val_ids))\n",
    "# len(set(os.listdir('../data/wav/')) - set(test_ids) - set(val_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving all the test and validation speakers to be reused throughout the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/test_ids.txt', 'w') as f:\n",
    "#     f.write(str(test_ids))\n",
    "\n",
    "# with open('../data/val_ids.txt', 'w') as f:\n",
    "#     f.write(str(val_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/test_ids.txt', 'r') as f:\n",
    "    test_ids = eval(f.read())\n",
    "\n",
    "with open('../data/val_ids.txt', 'r') as f:\n",
    "    val_ids = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_files = glob.glob('../data/wav/**/**/*.wav')\n",
    "samples_df = pd.DataFrame({'path': samples_files})\n",
    "samples_df['path'] = samples_df['path'].str.replace('\\\\', '/')\n",
    "samples_df['speaker_id'] = samples_df['path'].apply(lambda path: path.split('/')[-3])\n",
    "samples_df['utterance_id'] = samples_df['path'].apply(lambda path: path.split('/')[-2])\n",
    "samples_df['sample_id'] = samples_df['path'].apply(lambda path: path.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>sample_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/wav/id10001/1zcIwhmdeo4/00001.wav</td>\n",
       "      <td>id10001</td>\n",
       "      <td>1zcIwhmdeo4</td>\n",
       "      <td>00001.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/wav/id10001/1zcIwhmdeo4/00002.wav</td>\n",
       "      <td>id10001</td>\n",
       "      <td>1zcIwhmdeo4</td>\n",
       "      <td>00002.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/wav/id10001/1zcIwhmdeo4/00003.wav</td>\n",
       "      <td>id10001</td>\n",
       "      <td>1zcIwhmdeo4</td>\n",
       "      <td>00003.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/wav/id10001/7gWzIy6yIIk/00001.wav</td>\n",
       "      <td>id10001</td>\n",
       "      <td>7gWzIy6yIIk</td>\n",
       "      <td>00001.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/wav/id10001/7gWzIy6yIIk/00002.wav</td>\n",
       "      <td>id10001</td>\n",
       "      <td>7gWzIy6yIIk</td>\n",
       "      <td>00002.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153511</th>\n",
       "      <td>../data/wav/id11251/Tmh87G_cDZo/00004.wav</td>\n",
       "      <td>id11251</td>\n",
       "      <td>Tmh87G_cDZo</td>\n",
       "      <td>00004.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153512</th>\n",
       "      <td>../data/wav/id11251/WbB8m9-wlIQ/00001.wav</td>\n",
       "      <td>id11251</td>\n",
       "      <td>WbB8m9-wlIQ</td>\n",
       "      <td>00001.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153513</th>\n",
       "      <td>../data/wav/id11251/WbB8m9-wlIQ/00002.wav</td>\n",
       "      <td>id11251</td>\n",
       "      <td>WbB8m9-wlIQ</td>\n",
       "      <td>00002.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153514</th>\n",
       "      <td>../data/wav/id11251/XHCSVYEZvlM/00001.wav</td>\n",
       "      <td>id11251</td>\n",
       "      <td>XHCSVYEZvlM</td>\n",
       "      <td>00001.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153515</th>\n",
       "      <td>../data/wav/id11251/XHCSVYEZvlM/00002.wav</td>\n",
       "      <td>id11251</td>\n",
       "      <td>XHCSVYEZvlM</td>\n",
       "      <td>00002.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153516 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             path speaker_id utterance_id  \\\n",
       "0       ../data/wav/id10001/1zcIwhmdeo4/00001.wav    id10001  1zcIwhmdeo4   \n",
       "1       ../data/wav/id10001/1zcIwhmdeo4/00002.wav    id10001  1zcIwhmdeo4   \n",
       "2       ../data/wav/id10001/1zcIwhmdeo4/00003.wav    id10001  1zcIwhmdeo4   \n",
       "3       ../data/wav/id10001/7gWzIy6yIIk/00001.wav    id10001  7gWzIy6yIIk   \n",
       "4       ../data/wav/id10001/7gWzIy6yIIk/00002.wav    id10001  7gWzIy6yIIk   \n",
       "...                                           ...        ...          ...   \n",
       "153511  ../data/wav/id11251/Tmh87G_cDZo/00004.wav    id11251  Tmh87G_cDZo   \n",
       "153512  ../data/wav/id11251/WbB8m9-wlIQ/00001.wav    id11251  WbB8m9-wlIQ   \n",
       "153513  ../data/wav/id11251/WbB8m9-wlIQ/00002.wav    id11251  WbB8m9-wlIQ   \n",
       "153514  ../data/wav/id11251/XHCSVYEZvlM/00001.wav    id11251  XHCSVYEZvlM   \n",
       "153515  ../data/wav/id11251/XHCSVYEZvlM/00002.wav    id11251  XHCSVYEZvlM   \n",
       "\n",
       "        sample_id  \n",
       "0       00001.wav  \n",
       "1       00002.wav  \n",
       "2       00003.wav  \n",
       "3       00001.wav  \n",
       "4       00002.wav  \n",
       "...           ...  \n",
       "153511  00004.wav  \n",
       "153512  00001.wav  \n",
       "153513  00002.wav  \n",
       "153514  00001.wav  \n",
       "153515  00002.wav  \n",
       "\n",
       "[153516 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = samples_df.loc[~samples_df['speaker_id'].isin(test_ids) & ~samples_df['speaker_id'].isin(val_ids)]\n",
    "test_df = samples_df.loc[samples_df['speaker_id'].isin(test_ids)]\n",
    "val_df = samples_df.loc[samples_df['speaker_id'].isin(val_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filepath in tqdm(samples_df['path']):\n",
    "#     _, sr = torchaudio.load(filepath)\n",
    "#     assert sr == 16000, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_lengths = []\n",
    "# for filepath in tqdm(samples_df['path'][:2000]):\n",
    "#     audio, _ = torchaudio.load(filepath)\n",
    "#     audio_lengths.append(audio.size()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving in HDF5 format (created with ChatGPT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDF5_FILE = \"../data/dataset.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize HDF5 file\n",
    "# with h5py.File(HDF5_FILE, \"w\") as hf:\n",
    "#     # Iterate over speaker folders\n",
    "#     for speaker_id in tqdm(os.listdir('..\\\\data\\\\wav\\\\'), desc='Speakers'):\n",
    "#         speaker_group = hf.create_group(speaker_id)\n",
    "#         speaker_path = os.path.join('..\\\\data\\\\wav\\\\', speaker_id)\n",
    "#         # Iterate over utterance folders\n",
    "#         for utterance_id in os.listdir(speaker_path):\n",
    "#             utterance_group = speaker_group.create_group(utterance_id)\n",
    "#             utterance_path = os.path.join(speaker_path, utterance_id)\n",
    "#             # Iterate over sample files\n",
    "#             for sample_id in os.listdir(utterance_path):\n",
    "#                 sample_path = os.path.join(utterance_path, sample_id)\n",
    "#                 if not os.path.isfile(sample_path):\n",
    "#                     continue\n",
    "#                 # Read the sample data\n",
    "#                 data, sr = torchaudio.load(sample_path)\n",
    "#                 # Create a dataset in the HDF5 file and write the sample data\n",
    "#                 utterance_group.create_dataset(sample_id, data=data, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample(speaker_id, utterance_id, sample_id):\n",
    "    with h5py.File(HDF5_FILE, \"r\") as hf:\n",
    "        try:\n",
    "            sample_data = hf[speaker_id][utterance_id][sample_id][:]\n",
    "            return torch.tensor(sample_data)\n",
    "        except KeyError:\n",
    "            print(\"Sample not found.\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxCeleb1Triplet(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_df: pd.DataFrame, transforms = None, max_length: int = 240000):\n",
    "        self.df = train_df\n",
    "        self.transforms = transforms\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _crop_or_extend(self, sample):\n",
    "        if sample.size()[1] > self.max_length:\n",
    "            sample = sample[:, :self.max_length]\n",
    "        else:\n",
    "            sample = torch.cat((sample, torch.zeros((1, self.max_length - sample.size()[1]))), dim=1)\n",
    "        return sample\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        selected_row = self.df.iloc[id]\n",
    "        selected_speaker_id = selected_row['speaker_id']\n",
    "\n",
    "        positive_row = self.df.loc[(self.df['speaker_id'] == selected_speaker_id) & (self.df.index != id)].sample(1).iloc[0]\n",
    "\n",
    "        assert positive_row is not None, f\"There are now samples for the same speaker {selected_speaker_id}, row {id}\"\n",
    "\n",
    "        negative_row = self.df.loc[(self.df['speaker_id'] != selected_speaker_id)].sample(1).iloc[0]\n",
    "\n",
    "        assert negative_row is not None, f\"There are no negative samples\"\n",
    "\n",
    "        anchor_audio, anchor_sr = torchaudio.load(selected_row['path'])\n",
    "        pos_audio, pos_sr = torchaudio.load(positive_row['path'])\n",
    "        neg_audio, neg_sr = torchaudio.load(negative_row['path'])\n",
    "        assert anchor_sr == 16000 and pos_sr == 16000 and neg_sr == 16000\n",
    "        # anchor_audio = load_sample(selected_row['speaker_id'], selected_row['utterance_id'], selected_row['sample_id'])\n",
    "        # pos_audio = load_sample(positive_row['speaker_id'], positive_row['utterance_id'], positive_row['sample_id'])\n",
    "        # neg_audio = load_sample(negative_row['speaker_id'], negative_row['utterance_id'], negative_row['sample_id'])\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            anchor_audio = self.transforms(anchor_audio)\n",
    "            pos_audio = self.transforms(pos_audio)\n",
    "            neg_audio = self.transforms(neg_audio)\n",
    "        \n",
    "        anchor_audio = self._crop_or_extend(anchor_audio)\n",
    "        pos_audio = self._crop_or_extend(pos_audio)\n",
    "        neg_audio = self._crop_or_extend(neg_audio)\n",
    "        \n",
    "        return anchor_audio, pos_audio, neg_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = VoxCeleb1Triplet(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speaker_id_to_int(speaker_id: str):\n",
    "    return int(speaker_id[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxCeleb1Unary(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_df: pd.DataFrame, transforms = None, max_length: int = 240000, supporting_count: int = 3):\n",
    "        self.df = train_df\n",
    "        self.transforms = transforms\n",
    "        self.max_length = max_length\n",
    "        self.supporting_count = supporting_count\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _crop_or_extend(self, sample):\n",
    "        if sample.size()[1] > self.max_length:\n",
    "            sample = sample[:, :self.max_length]\n",
    "        else:\n",
    "            sample = torch.cat((sample, torch.zeros((1, self.max_length - sample.size()[1]))), dim=1)\n",
    "        return sample\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        selected_row = self.df.iloc[id]\n",
    "        selected_speaker_id = selected_row['speaker_id']\n",
    "        positive_rows = self.df.loc[(self.df['speaker_id'] == selected_speaker_id) & (self.df.index != id)] \\\n",
    "            .sample(self.supporting_count)\n",
    "\n",
    "        audio, sr = torchaudio.load(selected_row['path'])\n",
    "        assert sr == 16000\n",
    "        audios = [audio]\n",
    "        for i in range(self.supporting_count):\n",
    "            audio, sr = torchaudio.load(positive_rows.iloc[i]['path'])\n",
    "            assert sr == 16000\n",
    "            audios.append(audio)\n",
    "        # audio = load_sample(selected_row['speaker_id'], selected_row['utterance_id'], selected_row['sample_id'])\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            for i in range(self.supporting_count + 1):\n",
    "                audios[i] = self.transforms(audios[i])\n",
    "        \n",
    "        for i in range(self.supporting_count + 1):\n",
    "            audios[i] = self._crop_or_extend(audios[i])\n",
    "        \n",
    "        return torch.cat(audios), torch.ones(self.supporting_count + 1) * speaker_id_to_int(selected_speaker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VoxCeleb1Unary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0703,  0.0703,  0.0916,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0151, -0.0161, -0.0156,  ..., -0.0096, -0.0074, -0.0042],\n",
       "         [-0.0003, -0.0072,  0.0037,  ..., -0.0096, -0.0074, -0.0017],\n",
       "         [-0.0203, -0.0190, -0.0181,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " tensor([1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxCeleb1Validation(torch.utils.data.Dataset):\n",
    "    def __init__(self, val_df: pd.DataFrame, transforms = None, max_length: int = 240000):\n",
    "        self.df = val_df\n",
    "        self.transforms = transforms\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _crop_or_extend(self, sample):\n",
    "        if sample.size()[1] > self.max_length:\n",
    "            sample = sample[:, :self.max_length]\n",
    "        else:\n",
    "            sample = torch.cat((sample, torch.zeros((1, self.max_length - sample.size()[1]))), dim=1)\n",
    "        return sample\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        selected_row = self.df.iloc[id]\n",
    "        selected_speaker_id = selected_row['speaker_id']\n",
    "\n",
    "        res_class = np.random.choice((0, 1))\n",
    "\n",
    "        if res_class == 1:\n",
    "            rel_row = self.df.loc[(self.df['speaker_id'] == selected_speaker_id) & (self.df.index != id)].sample(1).iloc[0]\n",
    "        else:\n",
    "            rel_row = self.df.loc[(self.df['speaker_id'] != selected_speaker_id)].sample(1).iloc[0]\n",
    "\n",
    "        assert rel_row is not None, f\"There are now samples for the row {id}\"\n",
    "\n",
    "        anchor_audio, anchor_sr = torchaudio.load(selected_row['path'])\n",
    "        rel_audio, rel_sr = torchaudio.load(rel_row['path'])\n",
    "        assert anchor_sr == 16000 and rel_sr == 16000\n",
    "        # anchor_audio = load_sample(selected_row['speaker_id'], selected_row['utterance_id'], selected_row['sample_id'])\n",
    "        # rel_audio = load_sample(rel_row['speaker_id'], rel_row['utterance_id'], rel_row['sample_id'])\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            anchor_audio = self.transforms(anchor_audio)\n",
    "            rel_audio = self.transforms(rel_audio)\n",
    "        \n",
    "        anchor_audio = self._crop_or_extend(anchor_audio)\n",
    "        rel_audio = self._crop_or_extend(rel_audio)\n",
    "\n",
    "        return anchor_audio, rel_audio, res_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = VoxCeleb1Validation(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0692, 0.3032, 0.3854,  ..., 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([[-0.0972, -0.0992, -0.0992,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseCNN(nn.Module):\n",
    "    def __init__(self, backbone_name: str, backbone_pretrained: bool, res_dim: int, n_fft: int, hop_size: int, n_mels: int, mapper_dropout_p: float, power: float = 1.0, sr: int = 16000, logmel: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.melspec = nn.Sequential(torchaudio.transforms.Spectrogram(\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_size,\n",
    "            power=power,\n",
    "        ), torchaudio.transforms.MelScale(\n",
    "            n_mels=n_mels,\n",
    "            sample_rate=sr,\n",
    "            n_stft=n_fft // 2 + 1,\n",
    "            f_min=0,\n",
    "        ))\n",
    "        self.logmel = logmel\n",
    "\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone_name,\n",
    "            features_only=True,\n",
    "            pretrained=backbone_pretrained,\n",
    "            in_chans=1,\n",
    "            exportable=True\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.mapper = nn.Sequential(\n",
    "            nn.Dropout(p=mapper_dropout_p),\n",
    "            nn.Linear(self.backbone.feature_info.channels()[-1], res_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        specs = self.melspec(input)\n",
    "        if self.logmel:\n",
    "            specs = torch.log10(torch.clamp(specs, min=torch.tensor(1e-3)))\n",
    "\n",
    "        emb = self.backbone(specs)[-1]\n",
    "\n",
    "        bs, ch, _, _ = emb.shape\n",
    "        emb = self.pool(emb)\n",
    "        emb = emb.view(bs, ch)\n",
    "\n",
    "        emb = self.mapper(emb)\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.bias, bn2.num_batches_tracked, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    }
   ],
   "source": [
    "model = SiameseCNN('tf_efficientnet_b0.in1k', True, 128, 1024, 512, 128, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseHead(nn.Module):\n",
    "    def __init__(self, input_dim: int, drop_p=0.25):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=drop_p)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            self.dropout,\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            self.dropout,\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            self.dropout,\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_head = SiameseHead(input_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE*4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "model_head = model_head.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin: float = 1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    @staticmethod\n",
    "    def calc_euclidean(x1, x2):\n",
    "        return (x1 - x2).pow(2).sum(1)\n",
    "    \n",
    "    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:\n",
    "        distance_positive = TripletLoss.calc_euclidean(anchor, positive)\n",
    "        distance_negative = TripletLoss.calc_euclidean(anchor, negative)\n",
    "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/triplet-loss-advanced-intro-49a07b7d8905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchAllTtripletLoss(nn.Module):\n",
    "    \"\"\"Uses all valid triplets to compute Triplet loss\n",
    "    Args:\n",
    "        margin: Margin value in the Triplet Loss equation\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=1.0, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.eps = eps\n",
    "        # self.threshold = self.margin*100\n",
    "    \n",
    "    def euclidean_distance_matrix(self, x):\n",
    "        \"\"\"Efficient computation of Euclidean distance matrix\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, embedding_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Distance matrix of shape (batch_size, batch_size)\n",
    "        \"\"\"\n",
    "        # step 1 - compute the dot product\n",
    "\n",
    "        # shape: (batch_size, batch_size)\n",
    "        dot_product = torch.mm(x, x.t())\n",
    "\n",
    "        # step 2 - extract the squared Euclidean norm from the diagonal\n",
    "\n",
    "        # shape: (batch_size,)\n",
    "        squared_norm = torch.diag(dot_product)\n",
    "\n",
    "        # step 3 - compute squared Euclidean distances\n",
    "\n",
    "        # shape: (batch_size, batch_size)\n",
    "        distance_matrix = squared_norm.unsqueeze(0) - 2 * dot_product + squared_norm.unsqueeze(1)\n",
    "\n",
    "        # get rid of negative distances due to numerical instabilities\n",
    "        distance_matrix = F.relu(distance_matrix)\n",
    "\n",
    "        # step 4 - compute the non-squared distances\n",
    "        \n",
    "        # handle numerical stability\n",
    "        # derivative of the square root operation applied to 0 is infinite\n",
    "        # we need to handle by setting any 0 to eps\n",
    "        mask = (distance_matrix == 0.0).float()\n",
    "\n",
    "        # use this mask to set indices with a value of 0 to eps\n",
    "        distance_matrix = distance_matrix + mask * self.eps\n",
    "\n",
    "        # now it is safe to get the square root\n",
    "        distance_matrix = torch.sqrt(distance_matrix)\n",
    "\n",
    "        # undo the trick for numerical stability\n",
    "        distance_matrix = (1.0 - mask)*distance_matrix\n",
    "\n",
    "        return distance_matrix\n",
    "\n",
    "    def get_triplet_mask(self, labels):\n",
    "        \"\"\"compute a mask for valid triplets\n",
    "        Args:\n",
    "            labels: Batch of integer labels. shape: (batch_size,)\n",
    "        Returns:\n",
    "            Mask tensor to indicate which triplets are actually valid. Shape: (batch_size, batch_size, batch_size)\n",
    "            A triplet is valid if:\n",
    "            `labels[i] == labels[j] and labels[i] != labels[k]`\n",
    "            and `i`, `j`, `k` are different.\n",
    "        \"\"\"\n",
    "        # step 1 - get a mask for distinct indices\n",
    "\n",
    "        # shape: (batch_size, batch_size)\n",
    "        indices_equal = torch.eye(labels.size()[0], dtype=torch.bool, device=labels.device)\n",
    "        indices_not_equal = torch.logical_not(indices_equal)\n",
    "        # shape: (batch_size, batch_size, 1)\n",
    "        i_not_equal_j = indices_not_equal.unsqueeze(2)\n",
    "        # shape: (batch_size, 1, batch_size)\n",
    "        i_not_equal_k = indices_not_equal.unsqueeze(1)\n",
    "        # shape: (1, batch_size, batch_size)\n",
    "        j_not_equal_k = indices_not_equal.unsqueeze(0)\n",
    "        # Shape: (batch_size, batch_size, batch_size)\n",
    "        distinct_indices = torch.logical_and(torch.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "\n",
    "        # step 2 - get a mask for valid anchor-positive-negative triplets\n",
    "\n",
    "        # shape: (batch_size, batch_size)\n",
    "        labels_equal = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
    "        # shape: (batch_size, batch_size, 1)\n",
    "        i_equal_j = labels_equal.unsqueeze(2)\n",
    "        # shape: (batch_size, 1, batch_size)\n",
    "        i_equal_k = labels_equal.unsqueeze(1)\n",
    "        # shape: (batch_size, batch_size, batch_size)\n",
    "        valid_indices = torch.logical_and(i_equal_j, torch.logical_not(i_equal_k))\n",
    "\n",
    "        # step 3 - combine two masks\n",
    "        mask = torch.logical_and(distinct_indices, valid_indices)\n",
    "\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, embeddings, labels):\n",
    "        \"\"\"computes loss value.\n",
    "        Args:\n",
    "        embeddings: Batch of embeddings, e.g., output of the encoder. shape: (batch_size, embedding_dim)\n",
    "        labels: Batch of integer labels associated with embeddings. shape: (batch_size,)\n",
    "        Returns:\n",
    "        Scalar loss value.\n",
    "        \"\"\"\n",
    "        # step 1 - get distance matrix\n",
    "        # shape: (batch_size, batch_size)\n",
    "        distance_matrix = self.euclidean_distance_matrix(embeddings)\n",
    "\n",
    "        # step 2 - compute loss values for all triplets by applying broadcasting to distance matrix\n",
    "\n",
    "        # shape: (batch_size, batch_size, 1)\n",
    "        anchor_positive_dists = distance_matrix.unsqueeze(2)\n",
    "        # shape: (batch_size, 1, batch_size)\n",
    "        anchor_negative_dists = distance_matrix.unsqueeze(1)\n",
    "        # get loss values for all possible n^3 triplets\n",
    "        # shape: (batch_size, batch_size, batch_size)\n",
    "        triplet_loss = anchor_positive_dists - anchor_negative_dists + self.margin\n",
    "\n",
    "        # step 3 - filter out invalid or easy triplets by setting their loss values to 0\n",
    "\n",
    "        # shape: (batch_size, batch_size, batch_size)\n",
    "        mask = self.get_triplet_mask(labels)\n",
    "        \n",
    "        triplet_loss = triplet_loss * mask\n",
    "        # easy triplets have negative loss values\n",
    "        triplet_loss = F.relu(triplet_loss)\n",
    "\n",
    "        triplet_loss, _ = torch.max(triplet_loss, dim=1)\n",
    "        triplet_loss, _ = torch.max(triplet_loss, dim=1)\n",
    "\n",
    "        # step 4 - compute scalar loss value by averaging positive losses\n",
    "        num_positive_losses = (triplet_loss > self.eps).float().sum()\n",
    "        triplet_loss = triplet_loss.sum() / (num_positive_losses + self.eps)\n",
    "\n",
    "        # pos_mask = (labels.unsqueeze(0) == labels.unsqueeze(1)) & ~torch.eye(labels.size(0), dtype=torch.bool, device=labels.device)\n",
    "        # pos_ids = pos_mask.nonzero()\n",
    "        # pos_random_indices = pos_ids[torch.randperm(pos_ids.size(0))][:, 1]\n",
    "        # pos_dists = distance_matrix[pos_mask][pos_random_indices]\n",
    "\n",
    "        # neg_mask = labels.unsqueeze(0) != labels.unsqueeze(1)\n",
    "        # neg_ids = neg_mask.nonzero()\n",
    "        # neg_random_indices = neg_ids[torch.randperm(neg_ids.size(0))][:, 1]\n",
    "        # neg_dists = distance_matrix[neg_mask][neg_random_indices]\n",
    "\n",
    "        # self.threshold, optim_train_acc = 0.0, 0.0\n",
    "        # for dist in torch.cat((pos_dists, neg_dists), dim=0):\n",
    "        #     acc = (len(pos_dists[pos_dists < dist]) + len(neg_dists[neg_dists >= dist]))/(len(pos_dists) + len(neg_dists))\n",
    "        #     if acc > optim_train_acc:\n",
    "        #         optim_train_acc = acc\n",
    "        #         self.threshold = float(dist)\n",
    "\n",
    "        return triplet_loss #, (optim_train_acc, self.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "EVAL_EVERY_STEPS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW([\n",
    "                {'params': model.mapper.parameters(), 'lr': 1e-3},\n",
    "                {'params': model.backbone.parameters(), 'lr': 3e-4}\n",
    "            ])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS * np.ceil(len(train_dataset) / BATCH_SIZE), eta_min=1e-6)\n",
    "criterion = torch.jit.script(BatchAllTtripletLoss())\n",
    "optimizer_head = torch.optim.AdamW([\n",
    "                {'params': model_head.parameters(), 'lr': 3e-3},\n",
    "            ])\n",
    "scheduler_head = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_head, EPOCHS * np.ceil(len(train_dataset) / BATCH_SIZE), eta_min=1e-6)\n",
    "criterion_head = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 4.0], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_f1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 - Loss: 4.2141, Train Acc: 0.9524, Optim distance threshold: 7.7487\n",
      "\tVal Acc: 0.5198, Val precision: 0.9111, Val recall: 0.0358, Val F1: 0.0689\n",
      "\tSaving the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 21 - Loss: 2.9065, Train Acc: 0.9524, Optim distance threshold: 5.8084\n",
      "\tVal Acc: 0.5005, Val precision: 1.0000, Val recall: 0.0099, Val F1: 0.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 41 - Loss: 2.2163, Train Acc: 0.9583, Optim distance threshold: 5.5616\n",
      "\tVal Acc: 0.5036, Val precision: 1.0000, Val recall: 0.0091, Val F1: 0.0180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 61 - Loss: 1.9381, Train Acc: 0.9539, Optim distance threshold: 4.1400\n",
      "\tVal Acc: 0.5083, Val precision: 1.0000, Val recall: 0.0087, Val F1: 0.0173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 81 - Loss: 1.7792, Train Acc: 0.9571, Optim distance threshold: 3.2815\n",
      "\tVal Acc: 0.5194, Val precision: 1.0000, Val recall: 0.0134, Val F1: 0.0264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 101 - Loss: 1.6140, Train Acc: 0.9489, Optim distance threshold: 2.4967\n",
      "\tVal Acc: 0.5081, Val precision: 0.9500, Val recall: 0.0083, Val F1: 0.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 121 - Loss: 1.3736, Train Acc: 0.9539, Optim distance threshold: 1.6884\n",
      "\tVal Acc: 0.5068, Val precision: 0.9767, Val recall: 0.0181, Val F1: 0.0356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 141 - Loss: 1.2795, Train Acc: 0.9576, Optim distance threshold: 1.5252\n",
      "\tVal Acc: 0.5328, Val precision: 0.7774, Val recall: 0.1036, Val F1: 0.1828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSaving the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 161 - Loss: 1.2793, Train Acc: 0.9539, Optim distance threshold: 1.1166\n",
      "\tVal Acc: 0.5814, Val precision: 0.7295, Val recall: 0.2614, Val F1: 0.3848\n",
      "\tSaving the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 181 - Loss: 1.2022, Train Acc: 0.9554, Optim distance threshold: 0.9760\n",
      "\tVal Acc: 0.6375, Val precision: 0.6078, Val recall: 0.7437, Val F1: 0.6689\n",
      "\tSaving the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 201 - Loss: 1.2049, Train Acc: 0.9524, Optim distance threshold: 0.8161\n",
      "\tVal Acc: 0.5777, Val precision: 0.5432, Val recall: 0.9397, Val F1: 0.6884\n",
      "\tSaving the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 221 - Loss: 1.1589, Train Acc: 0.9524, Optim distance threshold: 0.7493\n",
      "\tVal Acc: 0.5198, Val precision: 0.5056, Val recall: 0.9960, Val F1: 0.6707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 241 - Loss: 1.1546, Train Acc: 0.9554, Optim distance threshold: 0.6431\n",
      "\tVal Acc: 0.5073, Val precision: 0.5013, Val recall: 1.0000, Val F1: 0.6678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 261 - Loss: 1.1306, Train Acc: 0.9524, Optim distance threshold: 0.5697\n",
      "\tVal Acc: 0.5047, Val precision: 0.5041, Val recall: 0.9996, Val F1: 0.6702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 281 - Loss: 1.1193, Train Acc: 0.9452, Optim distance threshold: 0.5305\n",
      "\tVal Acc: 0.4878, Val precision: 0.4878, Val recall: 1.0000, Val F1: 0.6557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 301 - Loss: 1.1017, Train Acc: 0.9568, Optim distance threshold: 0.4333\n",
      "\tVal Acc: 0.5090, Val precision: 0.5089, Val recall: 1.0000, Val F1: 0.6745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 321 - Loss: 1.1056, Train Acc: 0.9539, Optim distance threshold: 0.4592\n",
      "\tVal Acc: 0.4938, Val precision: 0.4938, Val recall: 1.0000, Val F1: 0.6612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 340/9002 [1:56:30<49:28:21, 20.56s/it]\n",
      "Epochs:   0%|          | 0/1 [1:56:30<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m     dists \u001b[38;5;241m=\u001b[39m TripletLoss\u001b[38;5;241m.\u001b[39mcalc_euclidean(anchor_out, rel_out)\n\u001b[0;32m     46\u001b[0m     labels_list\u001b[38;5;241m.\u001b[39mappend(labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m---> 47\u001b[0m     pred_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43m(\u001b[49m\u001b[43mdists\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthresh\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     49\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(pred_list)\n\u001b[0;32m     50\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(labels_list)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
    "    running_loss = []\n",
    "    for step, (audios, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        audios = audios.view((BATCH_SIZE*4, 1, 240000))\n",
    "        labels = labels.view((BATCH_SIZE*4))\n",
    "        audios = audios.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(audios)\n",
    "        \n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        out = out.detach()\n",
    "        B, C = out.size()\n",
    "\n",
    "        expanded_out1 = out.unsqueeze(0).expand(B, B, C)\n",
    "        expanded_out2 = out.unsqueeze(1).expand(B, B, C)\n",
    "\n",
    "        concatenated_pairs = torch.cat((expanded_out1, expanded_out2), dim=-1)\n",
    "\n",
    "        concatenated_pairs = concatenated_pairs.view(-1, 2*C)\n",
    "\n",
    "        labels1 = labels.unsqueeze(0).expand(B, B)  # Shape: (1, B, C)\n",
    "        labels2 = labels.unsqueeze(1).expand(B, B)  # Shape: (B, 1, C)\n",
    "\n",
    "        # Concatenate embeddings of all pairs\n",
    "        concatenated_labels = (labels1 == labels2).reshape(B * B)\n",
    "\n",
    "        optimizer_head.zero_grad()\n",
    "        out_head = model_head(concatenated_pairs)\n",
    "        \n",
    "        loss_head = criterion_head(out_head, concatenated_labels)\n",
    "        loss_head.backward()\n",
    "        optimizer_head.step()\n",
    "        scheduler_head.step()\n",
    "\n",
    "        if step % EVAL_EVERY_STEPS == 0:\n",
    "            model.eval()\n",
    "            model_head.eval()\n",
    "\n",
    "            pred_head = torch.argmax(out_head, dim=1).cpu().detach().numpy()\n",
    "\n",
    "            train_acc = accuracy_score(concatenated_labels, pred_head)\n",
    "            train_prec = precision_score(concatenated_labels, pred_head)\n",
    "            train_rec = recall_score(concatenated_labels, pred_head)\n",
    "            train_f1 = f1_score(concatenated_labels, pred_head)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_list = []\n",
    "                labels_list = []\n",
    "                for val_step, (anchor_audio, rel_audio, labels) in enumerate(val_loader):\n",
    "                    anchor_audio = anchor_audio.to(DEVICE)\n",
    "                    rel_audio = rel_audio.to(DEVICE)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    anchor_out = model(anchor_audio)\n",
    "                    rel_out = model(rel_audio)\n",
    "\n",
    "                    labels_list.append(labels.cpu().numpy())\n",
    "\n",
    "                    concatenated_pairs = torch.cat((anchor_out, rel_out), dim=-1)\n",
    "                    out_head = model_head(concatenated_pairs)\n",
    "\n",
    "                    pred_list.append(torch.argmax(out_head, dim=-1).cpu().numpy())\n",
    "                \n",
    "                preds = np.concatenate(pred_list)\n",
    "                labels = np.concatenate(labels_list)\n",
    "                val_acc = accuracy_score(labels, preds)\n",
    "                val_prec = precision_score(labels, preds)\n",
    "                val_rec = recall_score(labels, preds)\n",
    "                val_f1 = f1_score(labels, preds)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            print(f\"Step: {step+1} - Loss: {running_loss[-1]:.4f}, Train Acc: {optim_train_acc:.4f}, Optim distance threshold: {thresh:.4f}\")\n",
    "            print(f\"\\tVal Acc: {val_acc:.4f}, Val precision: {val_prec:.4f}, Val recall: {val_rec:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "                print('\\tSaving the model...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add:\n",
    "- [x] Checkpoints\n",
    "- [ ] Augmentations\n",
    "- [ ] Use better models\n",
    "- [ ] Online triplet loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
