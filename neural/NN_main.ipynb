{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese CNN training for speaker verification\n",
    "\n",
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.datasets import VoxCeleb1Verification\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = VoxCeleb1Verification('../data', download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split\n",
    "\n",
    "We need to split data in the clever way, leaving some speakers for validation, and not using them as well as some speakers from pre-defined test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# occs = dict()\n",
    "# for _, _, _, _, f1, f2 in tqdm(test_dataset):\n",
    "#     id1 = f1.split('-')[0]\n",
    "#     id2 = f2.split('-')[0]\n",
    "#     if id1 not in occs:\n",
    "#         occs[id1] = 0\n",
    "#     if id2 not in occs:\n",
    "#         occs[id2] = 0\n",
    "#     occs[id1] += 1\n",
    "#     occs[id2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speakers who are used in test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(occs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ids = sorted(list(occs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting validation speakers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_ids = np.random.choice(list(set(os.listdir('../data/wav/')) - set(test_ids)), size=40, replace=False)\n",
    "# val_ids  = sorted(list(val_ids))\n",
    "# len(set(os.listdir('../data/wav/')) - set(test_ids) - set(val_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving all the test and validation speakers to be reused throughout the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/test_ids.txt', 'w') as f:\n",
    "#     f.write(str(test_ids))\n",
    "\n",
    "# with open('../data/val_ids.txt', 'w') as f:\n",
    "#     f.write(str(val_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/test_ids.txt', 'r') as f:\n",
    "    test_ids = eval(f.read())\n",
    "\n",
    "with open('../data/val_ids.txt', 'r') as f:\n",
    "    val_ids = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_files = glob.glob('../data/wav/**/**/*.wav')\n",
    "samples_df = pd.DataFrame({'path': samples_files})\n",
    "samples_df['path'] = samples_df['path'].str.replace('\\\\', '/')\n",
    "samples_df['speaker_id'] = samples_df['path'].apply(lambda path: path.split('/')[-3])\n",
    "samples_df['utterance_id'] = samples_df['path'].apply(lambda path: path.split('/')[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>utterance_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/wav/id10001/1zcIwhmdeo4/00001.wav</td>\n",
       "      <td>id10001</td>\n",
       "      <td>1zcIwhmdeo4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/wav/id10001/1zcIwhmdeo4/00002.wav</td>\n",
       "      <td>id10001</td>\n",
       "      <td>1zcIwhmdeo4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/wav/id10001/1zcIwhmdeo4/00003.wav</td>\n",
       "      <td>id10001</td>\n",
       "      <td>1zcIwhmdeo4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/wav/id10001/7gWzIy6yIIk/00001.wav</td>\n",
       "      <td>id10001</td>\n",
       "      <td>7gWzIy6yIIk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/wav/id10001/7gWzIy6yIIk/00002.wav</td>\n",
       "      <td>id10001</td>\n",
       "      <td>7gWzIy6yIIk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153511</th>\n",
       "      <td>../data/wav/id11251/Tmh87G_cDZo/00004.wav</td>\n",
       "      <td>id11251</td>\n",
       "      <td>Tmh87G_cDZo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153512</th>\n",
       "      <td>../data/wav/id11251/WbB8m9-wlIQ/00001.wav</td>\n",
       "      <td>id11251</td>\n",
       "      <td>WbB8m9-wlIQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153513</th>\n",
       "      <td>../data/wav/id11251/WbB8m9-wlIQ/00002.wav</td>\n",
       "      <td>id11251</td>\n",
       "      <td>WbB8m9-wlIQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153514</th>\n",
       "      <td>../data/wav/id11251/XHCSVYEZvlM/00001.wav</td>\n",
       "      <td>id11251</td>\n",
       "      <td>XHCSVYEZvlM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153515</th>\n",
       "      <td>../data/wav/id11251/XHCSVYEZvlM/00002.wav</td>\n",
       "      <td>id11251</td>\n",
       "      <td>XHCSVYEZvlM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153516 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             path speaker_id utterance_id\n",
       "0       ../data/wav/id10001/1zcIwhmdeo4/00001.wav    id10001  1zcIwhmdeo4\n",
       "1       ../data/wav/id10001/1zcIwhmdeo4/00002.wav    id10001  1zcIwhmdeo4\n",
       "2       ../data/wav/id10001/1zcIwhmdeo4/00003.wav    id10001  1zcIwhmdeo4\n",
       "3       ../data/wav/id10001/7gWzIy6yIIk/00001.wav    id10001  7gWzIy6yIIk\n",
       "4       ../data/wav/id10001/7gWzIy6yIIk/00002.wav    id10001  7gWzIy6yIIk\n",
       "...                                           ...        ...          ...\n",
       "153511  ../data/wav/id11251/Tmh87G_cDZo/00004.wav    id11251  Tmh87G_cDZo\n",
       "153512  ../data/wav/id11251/WbB8m9-wlIQ/00001.wav    id11251  WbB8m9-wlIQ\n",
       "153513  ../data/wav/id11251/WbB8m9-wlIQ/00002.wav    id11251  WbB8m9-wlIQ\n",
       "153514  ../data/wav/id11251/XHCSVYEZvlM/00001.wav    id11251  XHCSVYEZvlM\n",
       "153515  ../data/wav/id11251/XHCSVYEZvlM/00002.wav    id11251  XHCSVYEZvlM\n",
       "\n",
       "[153516 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = samples_df.loc[~samples_df['speaker_id'].isin(test_ids) & ~samples_df['speaker_id'].isin(val_ids)]\n",
    "test_df = samples_df.loc[samples_df['speaker_id'].isin(test_ids)]\n",
    "val_df = samples_df.loc[samples_df['speaker_id'].isin(val_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filepath in tqdm(samples_df['path']):\n",
    "#     _, sr = torchaudio.load(filepath)\n",
    "#     assert sr == 16000, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 2125.33it/s]\n"
     ]
    }
   ],
   "source": [
    "audio_lengths = []\n",
    "for filepath in tqdm(samples_df['path'][:2000]):\n",
    "    audio, _ = torchaudio.load(filepath)\n",
    "    audio_lengths.append(audio.size()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxCeleb1Triplet(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_df: pd.DataFrame, transforms = None, max_length: int = 240000):\n",
    "        self.df = train_df\n",
    "        self.transforms = transforms\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _crop_or_extend(self, sample):\n",
    "        if sample.size()[1] > self.max_length:\n",
    "            sample = sample[:, :self.max_length]\n",
    "        else:\n",
    "            sample = torch.cat((sample, torch.zeros((1, self.max_length - sample.size()[1]))), dim=1)\n",
    "        return sample\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        selected_row = self.df.iloc[id]\n",
    "        selected_speaker_id = selected_row['speaker_id']\n",
    "\n",
    "        positive_row = self.df.loc[(self.df['speaker_id'] == selected_speaker_id) & (self.df.index != id)].sample(1).iloc[0]\n",
    "\n",
    "        assert positive_row is not None, f\"There are now samples for the same speaker {selected_speaker_id}, row {id}\"\n",
    "\n",
    "        negative_row = self.df.loc[(self.df['speaker_id'] != selected_speaker_id)].sample(1).iloc[0]\n",
    "\n",
    "        assert negative_row is not None, f\"There are no negative samples\"\n",
    "\n",
    "        anchor_audio, anchor_sr = torchaudio.load(selected_row['path'])\n",
    "        pos_audio, pos_sr = torchaudio.load(positive_row['path'])\n",
    "        neg_audio, neg_sr = torchaudio.load(negative_row['path'])\n",
    "        assert anchor_sr == 16000 and pos_sr == 16000 and neg_sr == 16000\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            anchor_audio = self.transforms(anchor_audio)\n",
    "            pos_audio = self.transforms(pos_audio)\n",
    "            neg_audio = self.transforms(neg_audio)\n",
    "        \n",
    "        anchor_audio = self._crop_or_extend(anchor_audio)\n",
    "        pos_audio = self._crop_or_extend(pos_audio)\n",
    "        neg_audio = self._crop_or_extend(neg_audio)\n",
    "        \n",
    "        return anchor_audio, pos_audio, neg_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VoxCeleb1Triplet(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0703, 0.0703, 0.0916,  ..., 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([[-0.0039,  0.0002, -0.0114,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " tensor([[0.0653, 0.0603, 0.0573,  ..., 0.0000, 0.0000, 0.0000]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxCeleb1Validation(torch.utils.data.Dataset):\n",
    "    def __init__(self, val_df: pd.DataFrame, transforms = None, max_length: int = 240000):\n",
    "        self.df = val_df\n",
    "        self.transforms = transforms\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _crop_or_extend(self, sample):\n",
    "        if sample.size()[1] > self.max_length:\n",
    "            sample = sample[:, :self.max_length]\n",
    "        else:\n",
    "            sample = torch.cat((sample, torch.zeros((1, self.max_length - sample.size()[1]))), dim=1)\n",
    "        return sample\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        selected_row = self.df.iloc[id]\n",
    "        selected_speaker_id = selected_row['speaker_id']\n",
    "\n",
    "        res_class = np.random.choice((0, 1))\n",
    "\n",
    "        if res_class == 1:\n",
    "            rel_row = self.df.loc[(self.df['speaker_id'] == selected_speaker_id) & (self.df.index != id)].sample(1).iloc[0]\n",
    "        else:\n",
    "            rel_row = self.df.loc[(self.df['speaker_id'] != selected_speaker_id)].sample(1).iloc[0]\n",
    "\n",
    "        assert rel_row is not None, f\"There are now samples for the row {id}\"\n",
    "\n",
    "        anchor_audio, anchor_sr = torchaudio.load(selected_row['path'])\n",
    "        rel_audio, rel_sr = torchaudio.load(rel_row['path'])\n",
    "        assert anchor_sr == 16000 and rel_sr == 16000\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            anchor_audio = self.transforms(anchor_audio)\n",
    "            rel_audio = self.transforms(rel_audio)\n",
    "        \n",
    "        anchor_audio = self._crop_or_extend(anchor_audio)\n",
    "        rel_audio = self._crop_or_extend(rel_audio)\n",
    "\n",
    "        return anchor_audio, rel_audio, res_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = VoxCeleb1Validation(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0692, 0.3032, 0.3854,  ..., 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([[0.0109, 0.0094, 0.0081,  ..., 0.0000, 0.0000, 0.0000]]),\n",
       " 0)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseCNN(nn.Module):\n",
    "    def __init__(self, backbone_name: str, backbone_pretrained: bool, res_dim: int, n_fft: int, hop_size: int, n_mels: int, mapper_dropout_p: float, power: float = 1.0, sr: int = 16000, logmel: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.melspec = nn.Sequential(torchaudio.transforms.Spectrogram(\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_size,\n",
    "            power=power,\n",
    "        ), torchaudio.transforms.MelScale(\n",
    "            n_mels=n_mels,\n",
    "            sample_rate=sr,\n",
    "            n_stft=n_fft // 2 + 1,\n",
    "            f_min=0,\n",
    "        ))\n",
    "        self.logmel = logmel\n",
    "\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone_name,\n",
    "            features_only=True,\n",
    "            pretrained=backbone_pretrained,\n",
    "            in_chans=1,\n",
    "            exportable=True\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.mapper = nn.Sequential(\n",
    "            nn.Dropout(p=mapper_dropout_p),\n",
    "            nn.Linear(self.backbone.feature_info.channels()[-1], res_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        specs = self.melspec(input)\n",
    "        if self.logmel:\n",
    "            specs = torch.log10(torch.clamp(specs, min=torch.tensor(1e-3)))\n",
    "\n",
    "        emb = self.backbone(specs)[-1]\n",
    "\n",
    "        bs, ch, _, _ = emb.shape\n",
    "        emb = self.pool(emb)\n",
    "        emb = emb.view(bs, ch)\n",
    "\n",
    "        emb = self.mapper(emb)\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.bias, bn2.num_batches_tracked, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    }
   ],
   "source": [
    "model = SiameseCNN('tf_efficientnet_b0.in1k', True, 128, 1024, 512, 128, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin: float = 1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    @staticmethod\n",
    "    def calc_euclidean(x1, x2):\n",
    "        return (x1 - x2).pow(2).sum(1)\n",
    "    \n",
    "    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:\n",
    "        distance_positive = TripletLoss.calc_euclidean(anchor, positive)\n",
    "        distance_negative = TripletLoss.calc_euclidean(anchor, negative)\n",
    "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "EVAL_EVERY_STEPS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW([\n",
    "                {'params': model.mapper.parameters(), 'lr': 1e-3},\n",
    "                {'params': model.backbone.parameters(), 'lr': 3e-4}\n",
    "            ])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS * np.ceil(len(train_dataset) / BATCH_SIZE), eta_min=1e-6)\n",
    "criterion = torch.jit.script(TripletLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_val_f1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 - Loss: 9.4519, Train Acc: 0.5781, Optim distance threshold: 120.9663\n",
      "\tVal Acc: 0.5540, Val precision: 0.5417, Val recall: 0.8619, Val F1: 0.6652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6 - Loss: 5.5284, Train Acc: 0.6406, Optim distance threshold: 82.2975\n",
      "\tVal Acc: 0.5540, Val precision: 0.5277, Val recall: 0.8314, Val F1: 0.6456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 11 - Loss: 12.5828, Train Acc: 0.5625, Optim distance threshold: 56.2323\n",
      "\tVal Acc: 0.5597, Val precision: 0.5765, Val recall: 0.2917, Val F1: 0.3874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 16 - Loss: 4.1824, Train Acc: 0.6875, Optim distance threshold: 75.3693\n",
      "\tVal Acc: 0.5739, Val precision: 0.5517, Val recall: 0.7356, Val F1: 0.6305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 21 - Loss: 10.2566, Train Acc: 0.5938, Optim distance threshold: 81.2796\n",
      "\tVal Acc: 0.5511, Val precision: 0.5083, Val recall: 0.9387, Val F1: 0.6595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 26 - Loss: 5.2370, Train Acc: 0.5938, Optim distance threshold: 62.1200\n",
      "\tVal Acc: 0.6080, Val precision: 0.6340, Val recall: 0.6474, Val F1: 0.6406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 31 - Loss: 3.9192, Train Acc: 0.6406, Optim distance threshold: 64.0749\n",
      "\tVal Acc: 0.6705, Val precision: 0.6717, Val recall: 0.7228, Val F1: 0.6963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 36 - Loss: 6.0486, Train Acc: 0.6094, Optim distance threshold: 65.6375\n",
      "\tVal Acc: 0.6477, Val precision: 0.6107, Val recall: 0.8371, Val F1: 0.7062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 41 - Loss: 4.5701, Train Acc: 0.6562, Optim distance threshold: 72.3815\n",
      "\tVal Acc: 0.5881, Val precision: 0.5284, Val recall: 0.9255, Val F1: 0.6727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 46 - Loss: 9.1276, Train Acc: 0.5625, Optim distance threshold: 68.3054\n",
      "\tVal Acc: 0.6023, Val precision: 0.5625, Val recall: 0.9205, Val F1: 0.6983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 51 - Loss: 3.3480, Train Acc: 0.6094, Optim distance threshold: 59.6725\n",
      "\tVal Acc: 0.6364, Val precision: 0.5939, Val recall: 0.8757, Val F1: 0.7078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 56 - Loss: 2.9017, Train Acc: 0.6562, Optim distance threshold: 58.8730\n",
      "\tVal Acc: 0.5824, Val precision: 0.5374, Val recall: 0.9349, Val F1: 0.6825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 61 - Loss: 1.9734, Train Acc: 0.7344, Optim distance threshold: 63.0717\n",
      "\tVal Acc: 0.5682, Val precision: 0.5224, Val recall: 0.9819, Val F1: 0.6820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 66 - Loss: 5.5519, Train Acc: 0.6250, Optim distance threshold: 55.3447\n",
      "\tVal Acc: 0.6193, Val precision: 0.5677, Val recall: 0.8882, Val F1: 0.6927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 71 - Loss: 3.2953, Train Acc: 0.7031, Optim distance threshold: 53.1777\n",
      "\tVal Acc: 0.6335, Val precision: 0.5802, Val recall: 0.9659, Val F1: 0.7249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/1 [33:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m dists \u001b[38;5;241m=\u001b[39m TripletLoss\u001b[38;5;241m.\u001b[39mcalc_euclidean(anchor_out, rel_out)\n\u001b[0;32m     47\u001b[0m labels_list\u001b[38;5;241m.\u001b[39mappend(labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m---> 48\u001b[0m pred_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43m(\u001b[49m\u001b[43mdists\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptim_thresh\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
    "    running_loss = []\n",
    "    for step, (anchor_audio, positive_audio, negative_audio) in enumerate(tqdm(train_loader, desc=\"Training\", leave=False)):\n",
    "        anchor_audio = anchor_audio.to(DEVICE)\n",
    "        positive_audio = positive_audio.to(DEVICE)\n",
    "        negative_audio = negative_audio.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        anchor_out = model(anchor_audio)\n",
    "        positive_out = model(positive_audio)\n",
    "        negative_out = model(negative_audio)\n",
    "        \n",
    "        loss = criterion(anchor_out, positive_out, negative_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        if step % EVAL_EVERY_STEPS == 0:\n",
    "            with torch.no_grad():\n",
    "                pos_dists = TripletLoss.calc_euclidean(anchor_out, positive_out)\n",
    "                neg_dists = TripletLoss.calc_euclidean(anchor_out, negative_out)\n",
    "\n",
    "            optim_thresh, optim_train_acc = 0, 0\n",
    "            for dist in torch.cat((pos_dists, neg_dists), dim=0):\n",
    "                acc = (len(pos_dists[pos_dists < dist]) + len(neg_dists[neg_dists >= dist]))/(len(pos_dists) + len(neg_dists))\n",
    "                if acc > optim_train_acc:\n",
    "                    optim_train_acc = acc\n",
    "                    optim_thresh = dist\n",
    "            \n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_list = []\n",
    "                labels_list = []\n",
    "                for val_step, (anchor_audio, rel_audio, labels) in enumerate(val_loader):\n",
    "                    anchor_audio = anchor_audio.to(DEVICE)\n",
    "                    rel_audio = rel_audio.to(DEVICE)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    anchor_out = model(anchor_audio)\n",
    "                    rel_out = model(rel_audio)\n",
    "\n",
    "                    dists = TripletLoss.calc_euclidean(anchor_out, rel_out)\n",
    "                    labels_list.append(labels.cpu().numpy())\n",
    "                    pred_list.append((dists < optim_thresh).cpu().numpy())\n",
    "                    # if val_step >= 10:\n",
    "                    #     break\n",
    "                \n",
    "                preds = np.concatenate(pred_list)\n",
    "                labels = np.concatenate(labels_list)\n",
    "                val_acc = accuracy_score(labels, preds)\n",
    "                val_prec = precision_score(labels, preds)\n",
    "                val_rec = recall_score(labels, preds)\n",
    "                val_f1 = f1_score(labels, preds)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            print(f\"Step: {step+1} - Loss: {running_loss[-1]:.4f}, Train Acc: {optim_train_acc:.4f}, Optim distance threshold: {optim_thresh:.4f}\")\n",
    "            print(f\"\\tVal Acc: {val_acc:.4f}, Val precision: {val_prec:.4f}, Val recall: {val_rec:.4f}, Val F1: {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add:\n",
    "- Checkpoints\n",
    "- Augmentations\n",
    "- Use better models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
