{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff399099",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (4.22.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (22.1.0)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (5.2.0)\n",
      "Requirement already satisfied: fastapi in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (0.110.0)\n",
      "Requirement already satisfied: ffmpy in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (0.3.2)\n",
      "Requirement already satisfied: gradio-client==0.13.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (0.13.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (0.21.4)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (6.4.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (2.1.1)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (3.7.2)\n",
      "Requirement already satisfied: numpy~=1.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (1.24.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (3.9.15)\n",
      "Requirement already satisfied: packaging in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (23.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (2.0.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (10.0.1)\n",
      "Requirement already satisfied: pydantic>=2.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (2.6.4)\n",
      "Requirement already satisfied: pydub in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (0.0.9)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (6.0)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (0.3.4)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer[all]<1.0,>=0.9 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (4.10.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio) (0.29.0)\n",
      "Requirement already satisfied: fsspec in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio-client==0.13.0->gradio) (2024.3.1)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from gradio-client==0.13.0->gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from altair<6.0,>=4.2.0->gradio) (4.17.3)\n",
      "Requirement already satisfied: toolz in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: anyio in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (3.5.0)\n",
      "Requirement already satisfied: certifi in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (1.0.4)\n",
      "Requirement already satisfied: idna in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (3.4)\n",
      "Requirement already satisfied: sniffio in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.19.3->gradio) (3.9.0)\n",
      "Requirement already satisfied: requests in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.19.3->gradio) (4.65.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from pydantic>=2.0->gradio) (2.16.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from typer[all]<1.0,>=0.9->gradio) (8.0.4)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from fastapi->gradio) (0.36.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.15.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (1.26.16)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mpalamariuk/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio speechbrain timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c710df5ac1762484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import librosa  # For audio processing\n",
    "import torch  # Assuming a PyTorch-based speaker verification model\n",
    "import os\n",
    "import gradio as gr\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "from speechbrain.inference.speaker import EncoderClassifier\n",
    "import torchaudio\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import timm\n",
    "import gc\n",
    "\n",
    "from torchaudio.datasets import VoxCeleb1Verification\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from IPython.display import display, Audio\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda import empty_cache\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "from torchaudio.functional import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8076beb5-d232-4cff-98df-e401129ceb3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_sound(file_path, new_sr=16000):\n",
    "    waveform, sr = torchaudio.load(file_path, normalize=True)\n",
    "    waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=new_sr)\n",
    "    return waveform, new_sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1395e44a-9311-4066-86e7-9ad46826beb9",
   "metadata": {},
   "source": [
    "# PLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c9e7639-9b04-43b0-a118-a38896bc20c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_likelyhood_score(enrollment_embeddings, test_embeddings, mu, F, Sigma):\n",
    "    inv_Sigma = np.linalg.inv(Sigma)\n",
    "    inv_phi = np.linalg.inv(F.T @ inv_Sigma @ F + np.eye(F.shape[1]))\n",
    "    scores = []\n",
    "\n",
    "    for enroll_emb, test_emb in zip(enrollment_embeddings, test_embeddings):\n",
    "        enroll_emb_cent = enroll_emb - mu\n",
    "        test_emb_cent = test_emb - mu\n",
    "        \n",
    "        v1 = inv_Sigma @ enroll_emb_cent\n",
    "        v2 = inv_Sigma @ test_emb_cent\n",
    "\n",
    "        t1 = v1.T @ inv_phi @ F.T @ v2\n",
    "        t2 = v1.T @ inv_phi @ v1 / 2\n",
    "        t3 = v2.T @ inv_phi @ v2 / 2\n",
    "        \n",
    "        score = t1 - t2 - t3\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64094ad1-ee31-43a0-826a-2f2c4426df6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PLDA_BASE_PATH = '../plda/models/'\n",
    "\n",
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-xvect-voxceleb\", savedir=\"pretrained_models/spkrec-xvect-voxceleb\")\n",
    "\n",
    "with open(PLDA_BASE_PATH + 'pca_model.pkl', 'rb') as file:\n",
    "    pca = pickle.load(file)\n",
    "\n",
    "plda_params = np.load(PLDA_BASE_PATH + 'xv-plda.npz')\n",
    "\n",
    "mean = plda_params['mean']\n",
    "Sigma = plda_params['Sigma']\n",
    "F = plda_params['F']\n",
    "\n",
    "THRESHOLD = -0.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52bd0461-9a81-43bc-8d02-116d7966ef48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def verify_plda(audio1, audio2):\n",
    "    emb1 = classifier.encode_batch(load_sound(audio1)[0])[0, 0].numpy()\n",
    "    emb2 = classifier.encode_batch(load_sound(audio2)[0])[0, 0].numpy()\n",
    "    \n",
    "    en_embeddings = np.array([emb1, emb2])\n",
    "    te_embeddings = np.array([emb2, emb1])\n",
    "\n",
    "    en_embeddings, te_embeddings = pca.transform(en_embeddings), pca.transform(te_embeddings)\n",
    "    \n",
    "    scores = log_likelyhood_score(en_embeddings, te_embeddings, mean, F, Sigma)\n",
    "    score = np.mean(scores)\n",
    "    \n",
    "    return score, score > THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec30489a-ef98-4a0f-a02b-bd70867b00a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio1 = os.path.join(os.path.dirname(\"example_sounds/\"), \"maksym_ukr_phone.wav\")\n",
    "audio2 = os.path.join(os.path.dirname(\"example_sounds/\"), \"maksym_eng_comp.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b01d153-a212-4269-ba5e-4d294c503acd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0970898249031573, True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify_plda(audio1, audio2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfd4c25-e847-47e1-ac63-32ccacb92f8e",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6f94950-2888-4ced-a3a2-76cf93cbd8b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SiameseCNN(nn.Module):\n",
    "    def __init__(self, backbone_name: str, backbone_pretrained: bool, res_dim: int, n_fft: int, hop_size: int, n_mels: int, mapper_dropout_p: float, power: float = 1.0, sr: int = 16000, logmel: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.melspec = nn.Sequential(torchaudio.transforms.Spectrogram(\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_size,\n",
    "            power=power,\n",
    "        ), torchaudio.transforms.MelScale(\n",
    "            n_mels=n_mels,\n",
    "            sample_rate=sr,\n",
    "            n_stft=n_fft // 2 + 1,\n",
    "            f_min=0,\n",
    "        ))\n",
    "\n",
    "        self.augm = nn.Sequential(\n",
    "            torchaudio.transforms.TimeMasking(time_mask_param=40, p=0.8, iid_masks=True),\n",
    "            torchaudio.transforms.FrequencyMasking(freq_mask_param=n_mels//5, iid_masks=True),\n",
    "            # ToComplexTensor(),\n",
    "            # RandomTimeStretch(max_size = MAX_LENGTH, n_freq=n_mels, hop_length=hop_size),\n",
    "        )\n",
    "\n",
    "        self.logmel = logmel\n",
    "\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone_name,\n",
    "            features_only=True,\n",
    "            pretrained=backbone_pretrained,\n",
    "            in_chans=1,\n",
    "            exportable=True\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.mapper = nn.Sequential(\n",
    "            nn.Dropout(p=mapper_dropout_p),\n",
    "            nn.Linear(self.backbone.feature_info.channels()[-1], res_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input, return_specs=False, return_augm=False):\n",
    "        specs = self.melspec(input)\n",
    "        if self.logmel:\n",
    "            specs = torch.log10(torch.clamp(specs, min=torch.tensor(1e-3, device=specs.device)))\n",
    "        \n",
    "        if return_specs:\n",
    "            return specs\n",
    "\n",
    "        if self.training:\n",
    "            specs = self.augm(specs)\n",
    "            specs = specs.type(torch.float32)\n",
    "        if return_augm:\n",
    "            return specs\n",
    "\n",
    "        # emb = self.backbone(input)[-1]\n",
    "        emb = self.backbone(specs)[-1]\n",
    "\n",
    "        bs, ch, _, _ = emb.shape\n",
    "        emb = self.pool(emb)\n",
    "        emb = emb.view(bs, ch)\n",
    "\n",
    "        emb = self.mapper(emb)\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23d97f0e-9004-4c4a-8932-b89905c69390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SiameseHead(nn.Module):\n",
    "    def __init__(self, input_dim: int, drop_p=0.25):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=drop_p)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            self.dropout,\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            self.dropout,\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            self.dropout,\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "439d8bc4-014e-4f00-a89e-91c02d50bff8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.bias, bn2.num_batches_tracked, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SiameseHead(\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (layers): Sequential(\n",
       "    (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Dropout(p=0.25, inplace=False)\n",
       "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Dropout(p=0.25, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Dropout(p=0.25, inplace=False)\n",
       "    (10): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "MODEL_BASE_PATH = '../neural/models/'\n",
    "\n",
    "model = SiameseCNN('tf_efficientnet_b0.in1k', True, EMBEDDING_DIM, 1024, 512, 128, 0.25, logmel=True)\n",
    "model.load_state_dict(torch.load(MODEL_BASE_PATH + \"efficientnet_1.0.8.pt\", map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "model_head = SiameseHead(input_dim=2*EMBEDDING_DIM)\n",
    "model_head.load_state_dict(torch.load(MODEL_BASE_PATH + \"head_efficientnet_1.0.8.pt\", map_location=torch.device('cpu')))\n",
    "model_head.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8b995c6-2108-4da5-b46b-cce71a121f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def verify_cnn(audio_path1, audio_path2):\n",
    "    audio_1, _ = load_sound(audio_path1)\n",
    "    audio_2, _ = load_sound(audio_path2)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        audio_1 = audio_1.unsqueeze(0).mean(dim=1, keepdim=True)\n",
    "        audio_2 = audio_2.unsqueeze(0).mean(dim=1, keepdim=True)\n",
    "        out_1 = model(audio_1)\n",
    "        out_2 = model(audio_2)\n",
    "        out_proba1 = torch.sigmoid(model_head(torch.cat((out_1, out_2), dim=-1)))\n",
    "        out_proba2 = torch.sigmoid(model_head(torch.cat((out_2, out_1), dim=-1)))\n",
    "\n",
    "    return ((out_proba1 + out_proba2) / 2).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "861a1846-6fd1-4946-8a54-5b5f9fc143d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_path1 = os.path.join(os.path.dirname(\"example_sounds/\"), \"maksym_eng_comp.wav\")\n",
    "audio_path2 = os.path.join(os.path.dirname(\"example_sounds/\"), \"maksym_ukr_phone.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3a4f284-f738-4f66-9c9a-6510309eca13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.560362"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify_cnn(audio_path1, audio_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66e711-da82-4e4b-a43a-573f1192fddb",
   "metadata": {},
   "source": [
    "# UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ab20d4a-d4c0-429f-812a-09b87a78532a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_spectrogram(audio, sr):\n",
    "    # Generate the spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=256, fmax=8000)\n",
    "    S_DB = librosa.power_to_db(S, ref=np.max)\n",
    "    \n",
    "    # Plot the spectrogram\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    img = librosa.display.specshow(S_DB, sr=sr, x_axis='time', y_axis='mel', fmax=8000, ax=ax)\n",
    "    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "    \n",
    "    # Convert the Matplotlib figure to a PIL Image and return it\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "    image = Image.open(buf)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "initial_id",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def speaker_verification(audio1, audio2, method=\"Probabilistic Linear Discriminant Analysis  (PLDA)\"):\n",
    "    \"\"\"Performs speaker verification using the chosen method.\n",
    "\n",
    "    Args:\n",
    "        audio (bytes): Audio data from user upload or microphone.\n",
    "        method (str, optional): Verification method (\"PLDA\" or \"Siamese CNN\"). Defaults to \"PLDA\".\n",
    "\n",
    "    Returns:\n",
    "        str: Verification result, including speaker name and similarity score.\n",
    "    \"\"\"\n",
    "    y1, sr1 = librosa.load(audio1, sr=16000)\n",
    "    y2, sr2 = librosa.load(audio2, sr=16000)\n",
    "    \n",
    "    \n",
    "    if method == \"Siamese Convolutional Neural Network (SCNN)\":\n",
    "        similarity_score = verify_cnn(audio1, audio2)\n",
    "        result_text = f\"\"\"\n",
    "        Method: {method}\n",
    "        Similaroty Score: {similarity_score*100:.0f}%\n",
    "        Threshold: 70%\n",
    "        \n",
    "        Verification Result: {\"Successful\" if similarity_score > 0.7 else \"Failed\"}.\n",
    "        \"\"\"\n",
    "        \n",
    "    elif method == \"Probabilistic Linear Discriminant Analysis  (PLDA)\":\n",
    "        score, verification_result = verify_plda(audio1, audio2)\n",
    "        \n",
    "        result_text = f\"\"\"\n",
    "        Method: {method}\n",
    "        Score: {score:.2f}\n",
    "        Threshold: {THRESHOLD:.2f}\n",
    "        \n",
    "        Verification Result: {\"Successful\" if verification_result else \"Failed\"}.\n",
    "        \"\"\"\n",
    "        \n",
    "    else:\n",
    "        similarity_score = \"Wrong method selected.\"\n",
    "\n",
    "    return generate_spectrogram(y1, sr1), generate_spectrogram(y2, sr2), result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25ac630b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks(title=\"Speaker Verification Problem\") as demo:\n",
    "    gr.Markdown(\"# Speaker Verification Problem\")\n",
    "    gr.Markdown(\"## Maksym Palamariuk, Andrii Shevtsov, and Artur Shevtsov\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            with gr.Group():\n",
    "                audio1 = gr.Audio(type=\"filepath\", label=\"Upload Audio Or Use Microphone\")\n",
    "                gr.Examples(\n",
    "                    examples=glob.glob(os.path.dirname(\"example_sounds/\") + \"/*.wav\"),\n",
    "                    inputs=[audio1]\n",
    "                ) \n",
    "                \n",
    "            with gr.Group():\n",
    "                audio2 = gr.Audio(type=\"filepath\", label=\"Upload Another Audio\")\n",
    "                gr.Examples(\n",
    "                    examples=glob.glob(os.path.dirname(\"example_sounds/\") + \"/*.wav\"),\n",
    "                    inputs=[audio2]\n",
    "                )\n",
    "\n",
    "\n",
    "            method = gr.Radio(choices=[\"Probabilistic Linear Discriminant Analysis  (PLDA)\", \"Siamese Convolutional Neural Network (SCNN)\"], label=\"Method\")\n",
    "\n",
    "            verify_button = gr.Button(\"Verify\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            spec1 = gr.Image(label=\"Spectrogram of the first audio\")\n",
    "            spec2 = gr.Image(label=\"Spectrogram of the second audio\")\n",
    "            output=gr.Textbox(label=\"Result\")\n",
    "\n",
    "        verify_button.click(fn=speaker_verification, \n",
    "                            inputs=[audio1, audio2, method], \n",
    "                            outputs=[spec1, spec2, output])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7276c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d0274-aa01-4cba-a5e6-51c36fb5d983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
